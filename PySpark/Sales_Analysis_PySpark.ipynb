{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b1c8f6-7477-4011-8052-55f424e0f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "THE PROBLEM-\n",
    "\n",
    "Sales Data Cleaning + Insights\n",
    "You’re given a CSV file named sales_data.csv (I’ll describe it below — or you can mock it). The business wants:\n",
    "\n",
    "Clean the data (handle missing/nulls).\n",
    "\n",
    "Calculate total and average sales per region.\n",
    "\n",
    "Find the top 2 salespeople per region based on total sales.\n",
    "\n",
    "Add a column saying \"High Performer\" if their total sales > 50,000.\n",
    "\n",
    "Return the cleaned, enriched DataFrame.\n",
    "\n",
    "Export the result to CSV\n",
    "\n",
    "Calculate average deals per salesperson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a785223-88ca-4f7c-8104-05ab6626db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry point for DataFrame and SQL in PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "# Syntax: SparkSession.builder.appName(\"AppName\").getOrCreate()\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"Sales_Data\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "013b85cd-ce7e-41ba-90bb-61d5d9fcec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file path\n",
    "# Syntax: csv_file_path = \"your_path_here\"\n",
    "csv_file_path = r\"C:\\Users\\Admin\\OneDrive\\Desktop\\Study\\Pyspark\\sales_data.csv\"\n",
    "\n",
    "# Read CSV as DataFrame\n",
    "# Syntax: spark.read.csv(path, header=True, inferSchema=True)\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a284819-b5ea-495c-9cee-f2449550acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Data: Remove rows with missing Region or Sales, fill null Deals with 0\n",
    "cleaned_df = df.dropna(subset=[\"Region\", \"Sales\"]).fillna({\"Deals\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daf8729f-1be8-4a4c-b367-a7dfed99d43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------------+\n",
      "|Region|TotalSalesPerRegion| AvgSalesPerRegion|\n",
      "+------+-------------------+------------------+\n",
      "|  East|              52000|17333.333333333332|\n",
      "|  West|              70000|23333.333333333332|\n",
      "| North|              28000|           28000.0|\n",
      "+------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate Total and Average Sales per Region\n",
    "from pyspark.sql.functions import sum as _sum, avg\n",
    "cleaned_df.groupBy(\"Region\").agg(\n",
    "    _sum(\"Sales\").alias(\"TotalSalesPerRegion\"),\n",
    "    avg(\"Sales\").alias(\"AvgSalesPerRegion\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5e5d7c0-51c7-4ecc-becd-c4bd49fd8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Sales per Salesperson per Region\n",
    "sales_by_person = cleaned_df.groupBy(\"Salesperson\", \"Region\").agg(\n",
    "    _sum(\"Sales\").alias(\"TotalSales\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65a997f2-a597-4b3e-8a07-122e46f8b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rank Salespeople within each Region using Window Function\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "window_spec = Window.partitionBy(\"Region\").orderBy(col(\"TotalSales\").desc())\n",
    "ranked_sales = sales_by_person.withColumn(\"Rank\", row_number().over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "451aaa70-d470-4387-9be7-56e944a38910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter Top 2 and Mark High Performers\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "final_df = ranked_sales.filter(col(\"Rank\") <= 2).withColumn(\n",
    "    \"HighPerformer\", when(col(\"TotalSales\") > 50000, \"Yes\").otherwise(\"No\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee1a60-6603-41fb-b178-8e0d4bdbf895",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final Output\n",
    "Top 2 salespeople per region\n",
    "Their total sales\n",
    "A “Yes”/“No” column saying if they're a HighPerformer (TotalSales > 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3139699d-ea06-4276-92d9-67cae07e7b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------+----+-------------+\n",
      "|Salesperson|Region|TotalSales|Rank|HighPerformer|\n",
      "+-----------+------+----------+----+-------------+\n",
      "|      Aarti|  East|     40000|   1|           No|\n",
      "|      Sneha|  East|     12000|   2|           No|\n",
      "|       Ravi| North|     28000|   1|           No|\n",
      "|     Ramesh|  West|     50000|   1|           No|\n",
      "|     Vikram|  West|     20000|   2|           No|\n",
      "+-----------+------+----------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3cdab87-4ba4-4c46-86ef-7e02a4ebb772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|Salesperson|AverageDeals|\n",
      "+-----------+------------+\n",
      "|      Aarti|         5.5|\n",
      "|       Ravi|         7.0|\n",
      "|     Ramesh|         4.0|\n",
      "|      Sneha|         3.0|\n",
      "|     Vikram|         5.0|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate Average Deals per Salesperson\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avg_deals = df.groupBy(\"Salesperson\").agg(avg(\"Deals\").alias(\"AverageDeals\"))\n",
    "avg_deals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b721d2a-fd38-4ff7-98bc-9f314ceeb5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Final DataFrame to CSV  \n",
    "# Syntax: df.write.option(\"header\", True).csv(\"path\") \n",
    "final_df.toPandas().to_csv(r\"C:\\Users\\Admin\\OneDrive\\Desktop\\Study\\Pyspark\\top_salespeople.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33d935-c89b-4519-b29f-a3bc31c5e04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
